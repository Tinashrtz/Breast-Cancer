{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement function C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeanscluster(X, k, mu, tol=1e-4, maxIter=100):\n",
    "    n, d = X.shape\n",
    "    C = np.zeros(n)  # Cluster assignments\n",
    "    for _ in range(maxIter):\n",
    "        prev_mu = mu.copy()\n",
    "        # Assign each data point to the nearest cluster\n",
    "        for i in range(n):\n",
    "            distances = np.linalg.norm(X[i] - mu, axis=1)\n",
    "            C[i] = np.argmin(distances)\n",
    "        # Update cluster centers\n",
    "        for j in range(k):\n",
    "            mu[j] = np.mean(X[C == j], axis=0)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(mu - prev_mu) < tol:\n",
    "            break\n",
    "    return C, mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این تابع یک الگوریتم خوشه بندی پیاده سازی میکند که ورودی های مطابق آنچه در صورت سوال توضیح داده شد هستند.ابتدا هر نقطه را به نزدیکترین مرکز خوشه نسبت میدهد و خوشه متناظر آنرا به آن نقطه اختصاص میدهد. سپس مراکز خوشه هارا به روز رسانی میکند. به این صورت که مرکز هر خوشه را برابر با میانگین داده های آن خوشه قرار میدهد.و در انتها بررسی میکند که مراکز خوشه ها تغیر کرده اند یا خیر.اگر تغیرات کمتر از مقدار تعیین شده باشد الگوریتم متوقف میشود و نتیجه نهایی خوشه بندی و مرکز خوشه هارا برمیگرداند"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\"breast_data.csv\")\n",
    "X = features_df.values\n",
    "target_df = pd.read_csv(\"breast_labels.csv\")\n",
    "y = target_df.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش دیتاست را لود کرده ایم و فیچر ها و تارگت را جدا کرده ایم . در این مساله تارگت به صورت باینری است پس در مساله طبقه بندی هستیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "برای نتیجه بهتر فیچر هارا استاندارد کرده ایم که این کار به کمک دو فرمول امکان پذیر است که ما از یکی از آنها به دلخواه در اینجا استفاده کرده ایم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2 #number of clusters\n",
    "maxIter=100\n",
    "\n",
    "# Fixing the starting point outside the loop\n",
    "mu = X_scaled[np.random.choice(X_scaled.shape[0], k, replace=False)]  \n",
    "\n",
    "# Perform k-means clustering\n",
    "C, centroids = kmeanscluster(X_scaled, k, mu, tol=1e-4, maxIter=maxIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در اینجا چون در مساله طبقه بندی باینری هستیم تعداد خوشه هارا برابر ۲ گذاشتیم . تعداد تکرار هاراهم به صورت تجربی برابر ۱۰۰ قرار دادیم. طبق بررسی های دستی من مقدار تکرار تاثیر زیادی بر روی دقت نداشت. سپس، مراکز اولیه خوشه‌ها به صورت تصادفی از بین  نقاط داده‌های استاندارد‌شده انتخاب می‌شوند و  تا پایان اجرای الگوریتم ثابت می‌مانند. این مراکز اولیه با استفاده از تابع نامپای رندوم از بین نقاط داده‌های استاندارد‌شده انتخاب می‌شوند "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9066901408450704 0.5135336980666892\n",
      "Accuracy of k-means clustering: 0.9066901408450704\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = accuracy_score(y, C)\n",
    "accuracy2 = accuracy_score(y, 1 - C)\n",
    "# Choose the better accuracy\n",
    "accuracy = max(accuracy1, accuracy2)\n",
    "print(accuracy,silhouette_score(X,y))\n",
    "print(\"Accuracy of k-means clustering:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "علت تعریف دقت به این صورت این اتفاق بود که گاهی دقت مدل ۹۱ درصد و گاهی ۹ درصد اعلام میشد. علت این اتفاق این بود که چون در مساله طبقه بندی باینری هستیم انگار لیبل ها جابه جا میشد. بنابراین دقت نهایی را ماکسیمم دو دقت در نظر گرفتیم."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 different starting points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "starting point : 0\n",
      "0.9049295774647887 0.5135336980666892\n",
      "-------------------\n",
      "starting point : 1\n",
      "0.9119718309859155 0.5135336980666892\n",
      "-------------------\n",
      "starting point : 2\n",
      "0.9119718309859155 0.5135336980666892\n",
      "-------------------\n",
      "starting point : 3\n",
      "0.9049295774647887 0.5135336980666892\n",
      "-------------------\n",
      "starting point : 4\n",
      "0.9119718309859155 0.5135336980666892\n"
     ]
    }
   ],
   "source": [
    "k = 2  # Number of clusters\n",
    "maxIter = 100\n",
    "num_starting_points = 5\n",
    "\n",
    "max_accuracy = 0\n",
    "best_C = None\n",
    "best_centroids = None\n",
    "\n",
    "for i in range(num_starting_points):\n",
    "    # Fixing the starting point outside the loop\n",
    "    mu = X_scaled[np.random.choice(X_scaled.shape[0], k, replace=False)]\n",
    "    # Perform k-means clustering\n",
    "    C, centroids = kmeanscluster(X_scaled, k, mu, tol=1e-4, maxIter=maxIter)\n",
    "    # Calculate accuracy of clustering\n",
    "    accuracy1 = accuracy_score(y, C)\n",
    "    accuracy2 = accuracy_score(y, 1 - C)\n",
    "    # Choose the better accuracy\n",
    "    accuracy = max(accuracy1, accuracy2)\n",
    "    print(\"-------------------\")\n",
    "    print(\"starting point :\",i)\n",
    "    print(accuracy,silhouette_score(X,y))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش الگوریتم قبلی را برای ۵ نقطه شروع متمایز که به صورت رندوم انتخاب شده اند ، بررسی کردیم. که این کار به کمک یک حلقه تکرار انجام شده است. همانظور که مشاهده میشود دقت های بدست آمده خیلی با یکدیگر تفاوتی ندارد و این نشان دهنده این است که الگوریتممان خیلی وابسته به نقطه شروع ای که تصادفی انتخابش میکنیم نیست"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem 4-question b- answer:\n",
    "\n",
    "My Observations:\n",
    "Running the k-means clustering algorithm multiple times with different starting points can lead to different clustering results.\n",
    "The accuracy of the clustering may vary depending on the starting points chosen.\n",
    "In some cases, the algorithm may converge to a suboptimal solution, resulting in lower accuracy.\n",
    "However, in general, the accuracy tends to be relatively consistent across different runs, especially if the dataset is well-separated and the clusters are distinct.\n",
    "Choosing an appropriate number of clusters (k) and optimizing other parameters such as tolerance and maximum iterations can also impact the clustering accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " using provided initial centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'mu_init'])\n",
      "Accuracy of k-means clustering using provided initial centers: 0.9066901408450704\n"
     ]
    }
   ],
   "source": [
    "# Load initial centers from init_mu.mat\n",
    "init_mu_data = loadmat(\"init_mu.mat\")\n",
    "\n",
    "# Inspect the keys in the loaded data\n",
    "print(init_mu_data.keys())\n",
    "init_mu = init_mu_data[\"mu_init\"]\n",
    "\n",
    "k = init_mu.shape[1]  # Number of clusters\n",
    "maxIter = 100\n",
    "\n",
    "# Perform k-means clustering using the provided initial centers\n",
    "C, centroids = kmeanscluster(X_scaled, k, init_mu.T, tol=1e-4, maxIter=maxIter)\n",
    "\n",
    "# Calculate accuracy of clustering\n",
    "accuracy1 = accuracy_score(y, C)\n",
    "accuracy2 = accuracy_score(y, 1 - C)\n",
    "\n",
    "# Choose the better accuracy\n",
    "accuracy = max(accuracy1, accuracy2)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy of k-means clustering using provided initial centers:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "این قطعه کد اقدام به خوشه‌بندی روی داده‌ها، با استفاده از مراکز خوشه‌های اولیه که در فایل متلب ارائه شده‌اند، میکند. این توضیح کاملتر این الگوریتم به صورت زیر است:\n",
    "در ایتدا مراکز اولیه را بارگذاری میکنیم. مراکز اولیه خوشه‌ها از یک فایل متلب با استفاده از تابع لودمت از کتابخانه سایپای بارگذاری می‌شوند. \n",
    "سپس داده های بارگذاری شده را بررسی میکنیم.  برای این کار کلیدهای داده‌های بارگذاری شده را چاپ کردم تا ساختار آنها را درک کنم.\n",
    "سپس مراکز اولیه را استخراج کردم و آنهارا در یک متغیر مطابق آنچا در کد پیاده سازی شده، ذخیره کردم \n",
    " تعداد خوشه ها از شکل آرایه بدست آمده تعیین می‌شود. این کار با استخراج تعداد ستون‌ها، با فرض اینکه هر ستون یک مرکز خوشه را نمایش می‌دهد، صورت می‌گیرد\n",
    "تعداد حداکثر تکرارها برای الگوریتم مانند قبل روی ۱۰۰ تکرار تنظیم شده است.\n",
    "به طور کلی، این قطعه کد مراکز اولیه خوشه‌بندی از یک فایل بارگذاری میکند، خوشه‌بندی را با استفاده از این مراکز اولیه انجام می‌دهد، سپس دقت خوشه‌بندی را محاسبه و چاپ می‌کند \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------\n",
    "problem 4 - question d- answer\n",
    "\n",
    "What happens if you initialize with the true centres, obtained after the true clustering?\n",
    "\n",
    "Perfect Clustering: If you initialize the centroids with the true centers obtained from the true clustering, the k-means algorithm might converge quickly to a solution where each data point is assigned to its correct cluster. This could result in high accuracy when evaluating the clustering against the true labels.\n",
    "\n",
    "Overestimation of Algorithm Performance: Using the true centers for initialization might artificially inflate the perceived performance of the k-means algorithm. It may not reflect the algorithm's actual performance on unseen data or in real-world scenarios where true centers are unknown.\n",
    "\n",
    "Assumption Violation: Initializing with true centers assumes perfect knowledge of the data distribution, which is usually not available in real-world scenarios. This violates the assumptions of unsupervised learning, where the algorithm should learn patterns and structures from the data itself rather than relying on external information.\n",
    "\n",
    "Robustness Testing: While initializing with true centers can be useful for testing the robustness of the algorithm or as a sanity check, it's essential to evaluate the algorithm's performance under more realistic conditions, such as using random initialization or other initialization methods commonly employed in k-means.\n",
    "\n",
    "In summary, initializing with true centers can lead to overfitting and biased evaluation of the algorithm's performance. It's more informative to evaluate the algorithm under typical conditions, such as using random initialization, to assess its generalization ability and robustness.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------\n",
    "problem 4- question e- answer :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you achieve better accuracy using another unsupervised learning method? What about\n",
    "a supervised one? Explain. \n",
    "\n",
    "To potentially achieve better accuracy using another unsupervised learning method or a supervised one, we need to consider the characteristics of the data, the nature of the problem, and the strengths and weaknesses of different algorithms. Here's how each scenario might unfold:\n",
    "\n",
    "Using Another Unsupervised Learning Method:\n",
    "\n",
    "Hierarchical Clustering: This method does not require specifying the number of clusters in advance and can capture hierarchical relationships between clusters. If the data exhibits hierarchical structure or varying cluster densities, hierarchical clustering might yield better results than k-means.\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN can identify clusters of arbitrary shapes and sizes and is robust to noise and outliers. If the clusters in the data have irregular shapes or varying densities, DBSCAN might outperform k-means.\n",
    "Gaussian Mixture Models (GMM): GMM assumes that the data is generated from a mixture of several Gaussian distributions. If the data is not well-separated or contains overlapping clusters, GMM might provide better results than k-means.\n",
    "Using a Supervised Learning Method:\n",
    "\n",
    "Classification Algorithms (e.g., Support Vector Machines, Random Forests, Neural Networks): By treating clustering as a classification problem and using the true labels for training, supervised learning algorithms can learn complex decision boundaries that might better separate the clusters in the data. However, this approach assumes the availability of labeled data for training, which might not always be feasible or practical.\n",
    "Semi-supervised Learning: In scenarios where only a subset of the data is labeled, semi-supervised learning methods can leverage both labeled and unlabeled data to improve clustering accuracy. Techniques such as self-training or co-training can be applied to iteratively refine the clustering results using the available labeled data.\n",
    "In summary, achieving better accuracy using another unsupervised learning method or a supervised one depends on various factors such as the data characteristics, problem complexity, availability of labeled data, and algorithm suitability. It's essential to consider these factors and experiment with different algorithms to determine the most appropriate approach for your specific task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
